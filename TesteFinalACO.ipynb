{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd8adf7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler \n",
    "from sklearn.metrics import classification_report, make_scorer, recall_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, precision_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real \n",
    "from sklearn.feature_selection import SelectKBest, f_classif \n",
    "from sklearn.utils import resample # üÜï Para o Bootstrap (IC)\n",
    "\n",
    "# =========================================================\n",
    "# üöÄ I. FUN√á√ïES DE M√âTRICAS PERSONALIZADAS E ESTAT√çSTICAS\n",
    "# =========================================================\n",
    "\n",
    "def calcular_phi(y_test, y_pred, y_proba):\n",
    "    \"\"\"üß† Calcula o √çndice de Confiabilidade de Previs√£o (pHi).\"\"\"\n",
    "    if y_proba is None:\n",
    "        return np.nan\n",
    "    \n",
    "    # Probabilidade da classe predita\n",
    "    y_proba_completo = np.array([\n",
    "        y_proba[i] if y_pred[i] == 1 else (1 - y_proba[i]) \n",
    "        for i in range(len(y_pred))\n",
    "    ])\n",
    "    \n",
    "    # M√©dia das probabilidades APENAS nos acertos (y_pred == y_test)\n",
    "    acertos = (y_pred == y_test)\n",
    "    probabilidades_acertos = y_proba_completo[acertos]\n",
    "    phi = np.mean(probabilidades_acertos) if len(probabilidades_acertos) > 0 else 0\n",
    "    return phi\n",
    "\n",
    "def calcular_irpn(y_test, y_pred):\n",
    "    \"\"\"üÜï Calcula o IRPN conforme solicitado: VP / (VP + FP).\"\"\"\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    # cm √© [[TN, FP], [FN, VP]]\n",
    "    FP = cm[0, 1]\n",
    "    VP = cm[1, 1]\n",
    "    \n",
    "    return VP / (VP + FP) if (VP + FP) != 0 else np.nan\n",
    "\n",
    "def calcular_intervalo_confianca_bootstrap(y_true, y_pred, metric_func, n_iterations=1000, alpha=0.95):\n",
    "    \"\"\"üìä Calcula o Intervalo de Confian√ßa (IC) por Bootstrap para m√©tricas.\"\"\"\n",
    "    stats = []\n",
    "    n_size = len(y_true)\n",
    "    indices = np.arange(n_size)\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        # Reamostragem com reposi√ß√£o\n",
    "        boot_indices = resample(indices, replace=True, n_samples=n_size)\n",
    "        \n",
    "        y_true_boot = y_true[boot_indices]\n",
    "        y_pred_boot = y_pred[boot_indices]\n",
    "        \n",
    "        # Ajuste para chamar a m√©trica corretamente (Recall exige pos_label)\n",
    "        if metric_func.__name__ == 'recall_score':\n",
    "             score = metric_func(y_true_boot, y_pred_boot, pos_label=1, zero_division=0)\n",
    "        else:\n",
    "             score = metric_func(y_true_boot, y_pred_boot)\n",
    "        stats.append(score)\n",
    "\n",
    "    stats = np.array(stats)\n",
    "    p_lower = (1.0 - alpha) / 2.0\n",
    "    lower_bound = np.quantile(stats, p_lower)\n",
    "    p_upper = alpha + p_lower\n",
    "    upper_bound = np.quantile(stats, p_upper)\n",
    "    \n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# =========================================================\n",
    "# üìà II. FUN√á√ÉO PRINCIPAL DE AVALIA√á√ÉO GR√ÅFICA E M√âTRICA\n",
    "# =========================================================\n",
    "\n",
    "def avaliar_modelo_classificacao(y_test, y_pred, y_proba=None, model_name=\"Modelo de Classifica√ß√£o\"):\n",
    "    \"\"\"Executa a Matriz de Confus√£o, Curva ROC e calcula todas as m√©tricas + IC.\"\"\"\n",
    "    print(f\"--- 11 & 12. Avalia√ß√£o Final do Modelo: {model_name} ---\")\n",
    "\n",
    "    # 1. Matriz de Confus√£o\n",
    "    print(\"\\n[Gr√°fico 1] Plotando Matriz de Confus√£o...\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    disp.plot(cmap=plt.cm.Blues, values_format='d') \n",
    "    plt.title(f'Matriz de Confus√£o - {model_name}')\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Curva ROC e AUC\n",
    "    roc_auc = np.nan\n",
    "    if y_proba is not None and len(np.unique(y_test)) == 2: \n",
    "        print(\"\\n[Gr√°fico 2] Plotando Curva ROC...\")\n",
    "        y_scores = y_proba\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (AUC = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Classificador Aleat√≥rio') \n",
    "        plt.title(f'Curva ROC - {model_name}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        print(f\"‚úÖ √Årea Sob a Curva ROC (AUC): {roc_auc:.4f}\")\n",
    "    \n",
    "    # 3. M√©tricas e Intervalo de Confian√ßa (IC 95%)\n",
    "    print(\"\\n--- Resultados Detalhados (IC 95% Bootstrap) ---\")\n",
    "\n",
    "    \n",
    "    # pHi\n",
    "    phi = calcular_phi(y_test, y_pred, y_proba)\n",
    "    print(f\"üß† **pHi (Confiabilidade): {phi:.4f}**\")\n",
    "    \n",
    "    # IRPN (M√©trica Solicitada)\n",
    "    irpn = calcular_irpn(y_test, y_pred)\n",
    "    ic_irpn_low, ic_irpn_high = calcular_intervalo_confianca_bootstrap(y_test, y_pred, calcular_irpn)\n",
    "    print(f\"üÜï **HIT (VP / (VP + FP)): {irpn:.4f}** (IC: [{ic_irpn_low:.4f}, {ic_irpn_high:.4f}])\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    return roc_auc \n",
    "\n",
    "# =========================================================\n",
    "\n",
    "# üêú III. SIMULA√á√ÉO MMAS (Feature Selection)\n",
    "# =========================================================\n",
    "def mmas_feature_selection(X, y, random_state=42):\n",
    "    \"\"\"Simula a sele√ß√£o de atributos pelo MMAS (SelectKBest).\"\"\"\n",
    "    k_features = int(X.shape[1] * 0.5) \n",
    "    selector = SelectKBest(score_func=f_classif, k=k_features)\n",
    "    selector.fit(X, y)\n",
    "    selected_mask = selector.get_support(indices=True)\n",
    "    print(f\" [MMAS Simulado/Ajustado] Selecionando {k_features} atributos.\")\n",
    "    return selected_mask\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# ‚öôÔ∏è IV. EXECU√á√ÉO DO FLUXO PRINCIPAL\n",
    "# =========================================================\n",
    "\n",
    "# --- VARI√ÅVEIS DE CONFIGURA√á√ÉO ---\n",
    "NOME_ARQUIVO_ENTRADA = \"BaseFinal(v3).csv\" \n",
    "NOME_COLUNA_TARGET = 'TEM_ARTRITE' \n",
    "# ---------------------------------\n",
    "\n",
    "try:\n",
    "    # 1. Carregamento e Divis√£o\n",
    "    print(f\"--- 1. Carregando a Base de Dados: {NOME_ARQUIVO_ENTRADA} ---\")\n",
    "    base = pd.read_csv(NOME_ARQUIVO_ENTRADA) \n",
    "    X_prev = base.drop(columns=[NOME_COLUNA_TARGET])\n",
    "    y_classe = base[NOME_COLUNA_TARGET]\n",
    "    X_treino, X_teste, y_treino, y_teste = train_test_split(\n",
    "        X_prev, y_classe, test_size=0.2, random_state=42, stratify=y_classe \n",
    "    )\n",
    "    y_teste_arr = y_teste.values # Convertido para array para uso consistente nas m√©tricas\n",
    "    print(f\"Classes Treino (Original): {Counter(y_treino)}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # 4. Pr√©-Processamento (Padroniza√ß√£o)\n",
    "    print(\"--- 4. Aplicando Pr√©-Processamento (Padroniza√ß√£o) ---\")\n",
    "    scaler = StandardScaler()\n",
    "    X_treino_scaled = scaler.fit_transform(X_treino)\n",
    "    X_teste_scaled = scaler.transform(X_teste) \n",
    "    X_treino_df = pd.DataFrame(X_treino_scaled, columns=X_treino.columns)\n",
    "    X_teste_df = pd.DataFrame(X_teste_scaled, columns=X_teste.columns)\n",
    "    y_treino = y_treino.reset_index(drop=True)\n",
    "    print(\"Padroniza√ß√£o conclu√≠da.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # 5. Balanceamento H√≠brido (RUS -> SMOTE)\n",
    "    print(\"--- 5. Balanceamento H√≠brido (RUS 0.1 -> SMOTE) ---\")\n",
    "    rus = RandomUnderSampler(sampling_strategy=0.1, random_state=42)\n",
    "    X_rus, y_rus = rus.fit_resample(X_treino_df, y_treino)\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_rus, y_rus) \n",
    "    print(f\"Classes Treino (AP√ìS H√≠brido): {Counter(y_resampled)}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # 6. Re-escalonamento (MinMaxScaler)\n",
    "    print(\"--- 6. Aplicando MinMax (Necess√°rio para BernoulliNB) ---\")\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    X_resampled_nb = minmax_scaler.fit_transform(X_resampled)\n",
    "    X_teste_nb = minmax_scaler.transform(X_teste_df) \n",
    "    X_resampled_nb_df = pd.DataFrame(X_resampled_nb, columns=X_resampled.columns)\n",
    "    X_teste_nb_df = pd.DataFrame(X_teste_nb, columns=X_teste_df.columns)\n",
    "    print(\"Re-escalonamento MinMax conclu√≠do.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # 7. Sele√ß√£o de Atributos com MMAS SIMULADO\n",
    "    print(\"--- 7. Sele√ß√£o de Atributos com MMAS SIMULADO ---\")\n",
    "    features_selected_indices = mmas_feature_selection(X=X_resampled_nb_df, y=y_resampled, random_state=42)\n",
    "    features_selecionadas = X_resampled_nb_df.columns[features_selected_indices].tolist()\n",
    "    print(f\"Atributos Selecionados: {len(features_selecionadas)} de {X_resampled_nb_df.shape[1]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # 8. FILTRAGEM DOS DATASETS\n",
    "    X_treino_final = X_resampled_nb_df[features_selecionadas]\n",
    "    X_teste_final = X_teste_nb_df[features_selecionadas]\n",
    "\n",
    "    # 9. Ajuste de Hiperpar√¢metros (BernoulliNB)\n",
    "    print(\"--- 9. Ajuste Autom√°tico de Hiperpar√¢metros (BernoulliNB) ---\")\n",
    "    \n",
    "    # üåü CORRE√á√ÉO: Usando booleanos (True/False), n√£o strings.\n",
    "    search_spaces = {\n",
    "        'alpha': Real(1e-5, 1000.0, prior='log-uniform'), \n",
    "        'fit_prior': [True, False]\n",
    "    }\n",
    "    \n",
    "    # Mantendo o scorer original (precision_score)\n",
    "    scorer = make_scorer(precision_score, pos_label=0) \n",
    "\n",
    "    bayes_search = BayesSearchCV(\n",
    "        estimator=BernoulliNB(), search_spaces=search_spaces, n_iter=100, scoring=scorer, cv=10, n_jobs=-1, random_state=42, verbose=0\n",
    "    )\n",
    "    bayes_search.fit(X_treino_final, y_resampled) \n",
    "    modelo_nb_best = bayes_search.best_estimator_\n",
    "\n",
    "    print(f\"\\n‚úÖ Melhores Par√¢metros (Precision) Encontrados: {bayes_search.best_params_}\")\n",
    "    print(f\"‚úÖ Melhor Precision (Valida√ß√£o Cruzada): {bayes_search.best_score_:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # 10. Avalia√ß√£o do Melhor Modelo no Teste\n",
    "    y_pred = modelo_nb_best.predict(X_teste_final)\n",
    "    y_pred_proba = modelo_nb_best.predict_proba(X_teste_final)[:, 1] \n",
    "\n",
    "    print(\"Relat√≥rio de Classifica√ß√£o (Conjunto de Teste):\")\n",
    "    print(classification_report(y_teste_arr, y_pred))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "    # 11 & 12. Avalia√ß√£o Gr√°fica e M√©trica (Com as novas m√©tricas e IC)\n",
    "    roc_auc = avaliar_modelo_classificacao(\n",
    "        y_test=y_teste_arr, \n",
    "        y_pred=y_pred, \n",
    "        y_proba=y_pred_proba, \n",
    "        model_name=\"BernoulliNB Otimizado\"\n",
    "    )\n",
    "\n",
    "    if not np.isnan(roc_auc):\n",
    "        print(f\"Valor final do AUC: {roc_auc:.4f}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERRO: O arquivo '{NOME_ARQUIVO_ENTRADA}' n√£o foi encontrado. Certifique-se de que 'BaseFinal(v3).csv' est√° no diret√≥rio correto.\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ERRO: Verifique se as bibliotecas necess√°rias (skopt, imblearn, etc.) est√£o instaladas. Erro: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Ocorreu um erro durante o processamento: {e}\")\n",
    "\n",
    "# %%\n",
    "# =========================================================\n",
    "# üöÄ I. FUN√á√ïES DE M√âTRICAS PERSONALIZADAS E ESTAT√çSTICAS\n",
    "# \n",
    "# ‚ö†Ô∏è NOTA: Essas fun√ß√µes devem ser definidas ANTES de serem chamadas.\n",
    "#          Se j√° estiverem no seu c√≥digo principal, esta se√ß√£o √© redundante.\n",
    "# =========================================================\n",
    "\n",
    "def calcular_phi(y_test, y_pred, y_proba):\n",
    "    \"\"\"üß† Calcula o √çndice de Confiabilidade de Previs√£o (pHi).\"\"\"\n",
    "    if y_proba is None or len(y_pred) != len(y_proba):\n",
    "        return np.nan\n",
    "    \n",
    "    # Probabilidade da classe predita\n",
    "    y_proba_completo = np.array([\n",
    "        y_proba[i] if y_pred[i] == 1 else (1 - y_proba[i]) \n",
    "        for i in range(len(y_pred))\n",
    "    ])\n",
    "    \n",
    "    # M√©dia das probabilidades APENAS nos acertos (y_pred == y_test)\n",
    "    acertos = (y_pred == y_test)\n",
    "    probabilidades_acertos = y_proba_completo[acertos]\n",
    "    phi = np.mean(probabilidades_acertos) if len(probabilidades_acertos) > 0 else 0\n",
    "    return phi\n",
    "\n",
    "def calcular_irpn(y_test, y_pred):\n",
    "    \"\"\"üÜï Calcula o IRPN conforme solicitado: VP / (VP + FP).\"\"\"\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    # cm √© [[TN, FP], [FN, VP]]\n",
    "    FP = cm[0, 1]\n",
    "    VP = cm[1, 1]\n",
    "    \n",
    "    return VP / (VP + FP) if (VP + FP) != 0 else np.nan\n",
    "\n",
    "def calcular_intervalo_confianca_bootstrap(y_true, y_pred, metric_func, n_iterations=1000, alpha=0.95):\n",
    "    \"\"\"üìä Calcula o Intervalo de Confian√ßa (IC) por Bootstrap para m√©tricas.\"\"\"\n",
    "    stats = []\n",
    "    n_size = len(y_true)\n",
    "    indices = np.arange(n_size)\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        # Reamostragem com reposi√ß√£o\n",
    "        boot_indices = np.random.choice(indices, size=n_size, replace=True)\n",
    "        \n",
    "        y_true_boot = y_true[boot_indices]\n",
    "        y_pred_boot = y_pred[boot_indices]\n",
    "        \n",
    "        # Ajuste para chamar a m√©trica corretamente (Recall exige pos_label)\n",
    "        if metric_func.__name__ == 'recall_score':\n",
    "             score = metric_func(y_true_boot, y_pred_boot, pos_label=1, zero_division=0)\n",
    "        else:\n",
    "             score = metric_func(y_true_boot, y_pred_boot)\n",
    "        stats.append(score)\n",
    "\n",
    "    stats = np.array(stats)\n",
    "    p_lower = (1.0 - alpha) / 2.0\n",
    "    lower_bound = np.quantile(stats, p_lower)\n",
    "    p_upper = alpha + p_lower\n",
    "    upper_bound = np.quantile(stats, p_upper)\n",
    "    \n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "def avaliar_modelo_classificacao(y_test, y_pred, y_proba=None, model_name=\"Modelo de Classifica√ß√£o\"):\n",
    "    \"\"\"Executa a Matriz de Confus√£o, Curva ROC e calcula todas as m√©tricas + IC.\"\"\"\n",
    "    print(f\"\\n\" + \"=\"*20 + \" AVALIA√á√ÉO DO MODELO \" + \"=\"*20)\n",
    "    print(f\"--- Modelo: {model_name} ---\")\n",
    "\n",
    "    # Garante que y_test √© um array numpy para bootstrap e consist√™ncia\n",
    "    y_test_arr = y_test.values if isinstance(y_test, pd.Series) else y_test\n",
    "\n",
    "    # 1. Matriz de Confus√£o\n",
    "    print(\"\\n[Gr√°fico 1] Matriz de Confus√£o:\")\n",
    "    cm = confusion_matrix(y_test_arr, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    disp.plot(cmap=plt.cm.Purples, values_format='d') # Alterei a cor para combinar com o DT\n",
    "    plt.title(f'Matriz de Confus√£o - {model_name}')\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Curva ROC e AUC\n",
    "    roc_auc = np.nan\n",
    "    if y_proba is not None and len(np.unique(y_test_arr)) == 2: \n",
    "        print(\"\\n[Gr√°fico 2] Curva ROC:\")\n",
    "        fpr, tpr, thresholds = roc_curve(y_test_arr, y_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.plot(fpr, tpr, color='purple', lw=2, label=f'Curva ROC (AUC = {roc_auc:.4f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Aleat√≥rio (AUC = 0.5)')\n",
    "        plt.title(f'Curva ROC - {model_name}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        print(f\"‚úÖ √Årea Sob a Curva ROC (AUC): {roc_auc:.4f}\")\n",
    "    \n",
    "    # 3. M√©tricas e Intervalo de Confian√ßa (IC 95%)\n",
    "    print(\"\\n--- Resultados Detalhados (IC 95% Bootstrap) ---\")\n",
    "    \n",
    "    \n",
    "    # pHi\n",
    "    phi = calcular_phi(y_test_arr, y_pred, y_proba)\n",
    "    print(f\"üß† **pHi (Confiabilidade): {phi:.4f}**\")\n",
    "    \n",
    "    # IRPN (M√©trica Solicitada)\n",
    "    irpn = calcular_irpn(y_test_arr, y_pred)\n",
    "    ic_irpn_low, ic_irpn_high = calcular_intervalo_confianca_bootstrap(y_test_arr, y_pred, calcular_irpn)\n",
    "    print(f\"üÜï **HIT (VP / (VP + VN)): {irpn:.4f}** (IC: [{ic_irpn_low:.4f}, {ic_irpn_high:.4f}])\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    return roc_auc \n",
    "\n",
    "# =========================================================\n",
    "# üå≥ Decision Tree Code Block (Decision Tree)\n",
    "# =========================================================\n",
    "\n",
    "# Importa√ß√µes necess√°rias para o novo bloco\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Categorical # J√° importado, mas garantindo aqui\n",
    "from sklearn.metrics import recall_score, make_scorer, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, classification_report\n",
    "from sklearn.tree import plot_tree # Nova importa√ß√£o necess√°ria para plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ‚ö†Ô∏è **IMPORTANTE:** O c√≥digo abaixo assume que as vari√°veis abaixo est√£o definidas:\n",
    "# X_resampled, y_resampled, X_teste_df, y_teste, X_resampled.columns\n",
    "\n",
    "# Convertendo y_teste para numpy array, se for Series, para consist√™ncia com o restante do c√≥digo\n",
    "y_teste_arr = y_teste.values if isinstance(y_teste, pd.Series) else y_teste\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# 17. Implementa√ß√£o da √Årvore de Decis√£o (Decision Tree - DT) com BayesSearchCV\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"--- 17. √Årvore de Decis√£o (DT) com BayesSearchCV e Precision ---\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 17.1. Definir o Espa√ßo de Busca (Search Space) usando dimens√µes do skopt\n",
    "search_spaces_dt = {\n",
    "    # max_depth: Profundidade m√°xima, crucial para controlar o overfitting\n",
    "    'max_depth': [4,5],\n",
    "    # min_samples_split: M√≠nimo de amostras para dividir um n√≥\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    # min_samples_leaf: M√≠nimo de amostras em um n√≥ folha\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "        \n",
    "    'max_features': [None, 'sqrt', 'log2', 0.2, 0.4, 0.6, 0.8],\n",
    "    # criterion: M√©trica de qualidade da divis√£o\n",
    "    'criterion': Categorical(['gini', 'entropy'])\n",
    "    # class_weight: A chave para lidar com o desbalanceamento\n",
    "}\n",
    "\n",
    "# 17.2. Definir a M√©trica de Otimiza√ß√£o (Precision para consist√™ncia com o NB)\n",
    "scorer_dt = make_scorer(precision_score, pos_label=0)\n",
    "\n",
    "# 17.3. Configurar e Rodar o BayesSearchCV\n",
    "dt_bayes_search = BayesSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),\n",
    "    search_spaces=search_spaces_dt,\n",
    "    n_iter=100,\n",
    "    scoring=scorer_dt,\n",
    "    cv=10,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Treinamento na base H√çBRIDA (balanceada e padronizada)\n",
    "dt_bayes_search.fit(X_resampled, y_resampled) \n",
    "\n",
    "# 17.4. Extrair o Melhor Modelo e Par√¢metros\n",
    "modelo_dt_best = dt_bayes_search.best_estimator_\n",
    "best_params_dt = dt_bayes_search.best_params_\n",
    "best_score_dt = dt_bayes_search.best_score_\n",
    "\n",
    "print(f\"\\n‚úÖ Melhores Par√¢metros DT (Precision) Encontrados: {best_params_dt}\")\n",
    "print(f\"‚úÖ Melhor Precision (Valida√ß√£o Cruzada DT): {best_score_dt:.4f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# 18. Avalia√ß√£o do Decision Tree no Conjunto de Teste\n",
    "# Avalia usando o conjunto de TESTE (X_teste_df) PADRONIZADO\n",
    "y_pred_dt = modelo_dt_best.predict(X_teste_df)\n",
    "y_pred_proba_dt = modelo_dt_best.predict_proba(X_teste_df)[:, 1] \n",
    "\n",
    "print(\"Relat√≥rio de Classifica√ß√£o DT no Conjunto de Teste (Desbalanceado):\")\n",
    "print(classification_report(y_teste_arr, y_pred_dt))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# 19. & 20. Avalia√ß√£o Completa com M√©tricas e IC (Substitui os blocos 19 e 20)\n",
    "# Utilizamos a fun√ß√£o 'avaliar_modelo_classificacao' para gerar gr√°ficos e m√©tricas\n",
    "roc_auc_dt_final = avaliar_modelo_classificacao(\n",
    "    y_test=y_teste_arr, \n",
    "    y_pred=y_pred_dt, \n",
    "    y_proba=y_pred_proba_dt, \n",
    "    model_name=\"Decision Tree Otimizada\"\n",
    ")\n",
    "print(f\"Valor final do AUC da DT: {roc_auc_dt_final:.4f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# 21. Gr√°fico Adicional: Visualiza√ß√£o da Import√¢ncia de Features\n",
    "print(\"\\n--- 21. Plotando Import√¢ncia de Features (Decision Tree) ---\")\n",
    "\n",
    "importances = modelo_dt_best.feature_importances_\n",
    "feature_names = X_resampled.columns\n",
    "\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False).head(15) \n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(feature_importances_df['Feature'], feature_importances_df['Importance'], color='darkorchid')\n",
    "plt.xlabel(\"Import√¢ncia de Feature\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Top 15 Features Mais Importantes (Decision Tree)\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 22. Plotagem da √Årvore de Decis√£o (Visualiza√ß√£o Detalhada)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"--- 22. Plotagem da √Årvore de Decis√£o ---\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "plt.figure(figsize=(30, 15)) \n",
    "\n",
    "plot_tree(\n",
    "    modelo_dt_best, \n",
    "    feature_names=feature_names.tolist(), \n",
    "    class_names=['0 (N√£o)', '1 (Sim)'], \n",
    "    filled=True, \n",
    "    rounded=True, \n",
    "    precision=2, \n",
    "    fontsize=10\n",
    ")\n",
    "\n",
    "plt.title(\"Visualiza√ß√£o Completa da √Årvore de Decis√£o Otimizada\", fontsize=18)\n",
    "plt.savefig('decision_tree_visualizacao.png') \n",
    "print(\"‚úÖ Gr√°fico da √Årvore de Decis√£o salvo como 'decision_tree_visualizacao.png'\")\n",
    "# plt.show() # Descomente para exibir em ambientes interativos\n",
    "\n",
    "# %%\n",
    "# =========================================================\n",
    "# üì¶ 0. IMPORTA√á√ïES E FUN√á√ïES DE C√ÅLCULO\n",
    "# =========================================================\n",
    "\n",
    "# Importa√ß√µes necess√°rias (Atualizadas)\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Categorical\n",
    "from sklearn.metrics import (\n",
    "    precision_score, make_scorer, f1_score, roc_auc_score, \n",
    "    confusion_matrix, ConfusionMatrixDisplay, roc_curve, \n",
    "    classification_report, auc, recall_score # Adicionado 'auc' e 'recall_score'\n",
    ") \n",
    "from sklearn.utils import resample # Importa resample para o bootstrap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# üìà I. FUN√á√ïES DE C√ÅLCULO DE M√âTRICAS AVAN√áADAS (pHI, HIT/IRPN, IC)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def calcular_phi(y_test, y_pred, y_proba):\n",
    "    \"\"\"üß† Calcula o √çndice de Confiabilidade de Previs√£o (pHi).\"\"\"\n",
    "    if y_proba is None:\n",
    "        return np.nan\n",
    "    \n",
    "    # y_proba √© a probabilidade da CLASSE 1 (coluna 1).\n",
    "    # Obtemos a probabilidade da CLASSE PREDITA.\n",
    "    y_proba_predita = np.array([\n",
    "        y_proba[i] if y_pred[i] == 1 else (1 - y_proba[i]) \n",
    "        for i in range(len(y_pred))\n",
    "    ])\n",
    "    \n",
    "    # M√©dia das probabilidades APENAS nos acertos (y_pred == y_test)\n",
    "    acertos = (y_pred == y_test)\n",
    "    probabilidades_acertos = y_proba_predita[acertos]\n",
    "    phi = np.mean(probabilidades_acertos) if len(probabilidades_acertos) > 0 else 0\n",
    "    return phi\n",
    "\n",
    "def calcular_irpn(y_test, y_pred):\n",
    "    \"\"\"üÜï Calcula o IRPN conforme solicitado: VP / (VP + FP).\"\"\"\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    # cm √© [[TN, FP], [FN, VP]]\n",
    "    FP = cm[0, 1]\n",
    "    VP = cm[1, 1]\n",
    "    \n",
    "    return VP / (VP + FP) if (VP + FP) != 0 else np.nan\n",
    "\n",
    "def calcular_intervalo_confianca_bootstrap(y_true, y_pred, metric_func, n_iterations=1000, alpha=0.95, y_proba=None):\n",
    "    \"\"\"üìä Calcula o Intervalo de Confian√ßa (IC) por Bootstrap para m√©tricas.\"\"\"\n",
    "    stats = []\n",
    "    n_size = len(y_true)\n",
    "    indices = np.arange(n_size)\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        # Reamostragem com reposi√ß√£o\n",
    "        boot_indices = resample(indices, replace=True, n_samples=n_size)\n",
    "        \n",
    "        y_true_boot = y_true[boot_indices]\n",
    "        y_pred_boot = y_pred[boot_indices]\n",
    "        \n",
    "        # Ajuste para chamar a m√©trica corretamente, incluindo pHi\n",
    "        if metric_func.__name__ == 'calcular_phi':\n",
    "            y_proba_boot = y_proba[boot_indices] if y_proba is not None else None\n",
    "            score = metric_func(y_true_boot, y_pred_boot, y_proba_boot)\n",
    "        elif metric_func.__name__ == 'recall_score':\n",
    "            score = recall_score(y_true_boot, y_pred_boot, pos_label=1, zero_division=0)\n",
    "        elif metric_func.__name__ == 'f1_score':\n",
    "            score = f1_score(y_true_boot, y_pred_boot, pos_label=1, zero_division=0)\n",
    "        else:\n",
    "            score = metric_func(y_true_boot, y_pred_boot)\n",
    "        \n",
    "        stats.append(score)\n",
    "\n",
    "    stats = np.array(stats)\n",
    "    p_lower = (1.0 - alpha) / 2.0\n",
    "    lower_bound = np.quantile(stats, p_lower)\n",
    "    p_upper = alpha + p_lower\n",
    "    upper_bound = np.quantile(stats, p_upper)\n",
    "    \n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# üìà II. FUN√á√ÉO PRINCIPAL DE AVALIA√á√ÉO GR√ÅFICA E M√âTRICA\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def avaliar_modelo_classificacao(y_test, y_pred, y_proba=None, model_name=\"Modelo de Classifica√ß√£o\"):\n",
    "    \"\"\"Executa a Matriz de Confus√£o, Curva ROC e calcula todas as m√©tricas + IC.\"\"\"\n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    print(f\"--- Avalia√ß√£o Final do Modelo: {model_name} ---\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # 1. Relat√≥rio de Classifica√ß√£o (vis√£o geral)\n",
    "    print(\"Relat√≥rio de Classifica√ß√£o (M√©tricas Padr√£o):\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 2. Matriz de Confus√£o\n",
    "    print(\"\\n[Gr√°fico 1] Plotando Matriz de Confus√£o...\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    disp.plot(cmap=plt.cm.Greens, values_format='d')\n",
    "    plt.title(f'Matriz de Confus√£o - {model_name}')\n",
    "    plt.show() # \n",
    "    \n",
    "    # 3. Curva ROC e AUC\n",
    "    roc_auc = np.nan\n",
    "    if y_proba is not None and len(np.unique(y_test)) == 2: \n",
    "        print(\"\\n[Gr√°fico 2] Plotando Curva ROC...\")\n",
    "        y_scores = y_proba\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "        roc_auc = auc(fpr, tpr) \n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.plot(fpr, tpr, color='green', lw=2, label=f'Curva ROC (AUC = {roc_auc:.4f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Classificador Aleat√≥rio') \n",
    "        plt.title(f'Curva ROC - {model_name}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True)\n",
    "        plt.show() # \n",
    "        print(f\"‚úÖ √Årea Sob a Curva ROC (AUC): {roc_auc:.4f}\")\n",
    "    \n",
    "    # 4. M√©tricas Avan√ßadas e Intervalo de Confian√ßa (IC 95%)\n",
    "    print(\"\\n--- Resultados de Confiabilidade e Customizados (IC 95% Bootstrap) ---\")\n",
    "\n",
    "    # pHi (Confiabilidade/Confian√ßa)\n",
    "    phi = calcular_phi(y_test, y_pred, y_proba)\n",
    "    # IC para pHi (passando y_proba para a fun√ß√£o bootstrap)\n",
    "    ic_phi_low, ic_phi_high = calcular_intervalo_confianca_bootstrap(y_test, y_pred, calcular_phi, y_proba=y_proba)\n",
    "    print(f\"üß† **pHi (Confiabilidade): {phi:.4f}** (IC: [{ic_phi_low:.4f}, {ic_phi_high:.4f}])\")\n",
    "    \n",
    "    # IRPN (HIT)\n",
    "    irpn = calcular_irpn(y_test, y_pred)\n",
    "    ic_irpn_low, ic_irpn_high = calcular_intervalo_confianca_bootstrap(y_test, y_pred, calcular_irpn)\n",
    "    print(f\"üÜï **HIT (VP / (VP + VN)): {irpn:.4f}** (IC: [{ic_irpn_low:.4f}, {ic_irpn_high:.4f}])\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    return roc_auc \n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# 17. Implementa√ß√£o da Extra Trees (ET) com BayesSearchCV\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"--- 17. Extra Trees (ET) com BayesSearchCV e F1-Score ---\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 17.1. Definir o Espa√ßo de Busca (Search Space) - Adaptado para Extra Trees\n",
    "search_spaces_et = {\n",
    "    'n_estimators': [50, 75, 90, 100, 120],\n",
    "    'max_depth': [3, 4, 5, 7, 10], \n",
    "    'min_samples_split': [2, 4, 7, 10, 12, 15, 17, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 7, 10],\n",
    "    'max_features': ['sqrt', 'log2', None, 0.8, 0.6, 0.4, 0.2],\n",
    "    'criterion': Categorical(['gini', 'entropy'])\n",
    "}\n",
    "\n",
    "# 17.2. Definir a M√©trica de Otimiza√ß√£o (F1-Score para equil√≠brio)\n",
    "scorer_et = make_scorer(f1_score, pos_label=1) \n",
    "\n",
    "# 17.3. Configurar e Rodar o BayesSearchCV\n",
    "et_bayes_search = BayesSearchCV(\n",
    "    estimator=ExtraTreesClassifier(random_state=42, class_weight='balanced'), \n",
    "    search_spaces=search_spaces_et,\n",
    "    n_iter=100, \n",
    "    scoring=scorer_et,\n",
    "    cv=10,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Treinamento na base H√çBRIDA (balanceada e padronizada)\n",
    "# ASSUME-SE QUE X_resampled E y_resampled EST√ÉO DISPON√çVEIS\n",
    "try:\n",
    "    et_bayes_search.fit(X_resampled, y_resampled) \n",
    "except NameError:\n",
    "    print(\"ERRO: As vari√°veis 'X_resampled' e 'y_resampled' n√£o foram definidas. O treinamento n√£o pode ser realizado.\")\n",
    "    # Aqui o c√≥digo pararia ou usaria dados fict√≠cios de exemplo se fossem definidos\n",
    "    # Continue o fluxo se as vari√°veis estiverem definidas.\n",
    "    \n",
    "# 17.4. Extrair o Melhor Modelo e Par√¢metros\n",
    "modelo_et_best = et_bayes_search.best_estimator_\n",
    "best_params_et = et_bayes_search.best_params_\n",
    "best_score_et = et_bayes_search.best_score_\n",
    "\n",
    "print(f\"\\n‚úÖ Melhores Par√¢metros ET (F1-Score) Encontrados: {best_params_et}\")\n",
    "print(f\"‚úÖ Melhor F1-Score (Valida√ß√£o Cruzada ET): {best_score_et:.4f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# 18. Avalia√ß√£o do Extra Trees no Conjunto de Teste (CORRIGIDO)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# 18.1. Previs√µes\n",
    "try:\n",
    "    y_pred_et = modelo_et_best.predict(X_teste_df)\n",
    "    y_pred_proba_et = modelo_et_best.predict_proba(X_teste_df)[:, 1] # Probabilidade da classe positiva (1)\n",
    "except NameError:\n",
    "    print(\"ERRO: As vari√°veis 'X_teste_df' e 'y_teste' n√£o foram definidas. A avalia√ß√£o n√£o pode ser realizada.\")\n",
    "    raise\n",
    "\n",
    "# >>> CORRE√á√ÉO CR√çTICA: Converter para NumPy arrays antes de chamar a fun√ß√£o de avalia√ß√£o <<<\n",
    "# Isso garante que o indexador interno da fun√ß√£o de bootstrap funcione corretamente (baseado em posi√ß√£o, e n√£o em √≠ndice do Pandas).\n",
    "\n",
    "# Converte y_teste para array, se for uma Pandas Series.\n",
    "# A linha abaixo garante que mesmo que y_teste seja um array numpy, ele √© mantido como um array.\n",
    "y_teste_arr = np.array(y_teste) if isinstance(y_teste, (pd.Series, pd.DataFrame)) else y_teste\n",
    "y_pred_et_arr = np.array(y_pred_et) # y_pred √© geralmente um array numpy, mas garantimos\n",
    "y_pred_proba_et_arr = np.array(y_pred_proba_et) # Garante que as probabilidades s√£o um array\n",
    "\n",
    "# 18.2. Chamada da Fun√ß√£o de Avalia√ß√£o\n",
    "# Use as vari√°veis convertidas.\n",
    "avaliar_modelo_classificacao(\n",
    "    y_test=y_teste_arr, \n",
    "    y_pred=y_pred_et_arr, \n",
    "    y_proba=y_pred_proba_et_arr, \n",
    "    model_name=\"Extra Trees Otimizada\"\n",
    ")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# 21. Gr√°fico Adicional: Plotando Import√¢ncia de Features (Extra Trees)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"--- 21. Plotando Import√¢ncia de Features (Extra Trees) ---\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Extra Trees, como um modelo de ensemble de √°rvores, possui Feature Importance\n",
    "importances_et = modelo_et_best.feature_importances_\n",
    "# ASSUME-SE QUE X_resampled EST√Å DISPON√çVEL\n",
    "try:\n",
    "    feature_names = X_resampled.columns\n",
    "except AttributeError:\n",
    "    # Caso X_resampled seja um np.array e n√£o um DataFrame\n",
    "    print(\"Aviso: 'X_resampled.columns' n√£o est√° dispon√≠vel. Usando √≠ndices num√©ricos para features.\")\n",
    "    feature_names = [f'Feature {i}' for i in range(len(importances_et))]\n",
    "except NameError:\n",
    "    print(\"ERRO: Vari√°vel 'X_resampled' n√£o foi definida. N√£o √© poss√≠vel calcular a import√¢ncia das features.\")\n",
    "    raise\n",
    "\n",
    "\n",
    "feature_importances_df_et = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances_et\n",
    "}).sort_values(by='Importance', ascending=False).head(15) \n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(feature_importances_df_et['Feature'], feature_importances_df_et['Importance'], color='seagreen')\n",
    "plt.xlabel(\"Import√¢ncia de Feature\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Top 15 Features Mais Importantes (Extra Trees)\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show() #\n",
    "\n",
    "# %%\n",
    "# =========================================================\n",
    "# üì¶ 0. IMPORTA√á√ïES E FUN√á√ïES DE C√ÅLCULO\n",
    "# =========================================================\n",
    "\n",
    "# Importa√ß√µes necess√°rias (Atualizadas)\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, f1_score, roc_auc_score, confusion_matrix, \n",
    "    ConfusionMatrixDisplay, roc_curve, precision_score, \n",
    "    classification_report, auc, recall_score # Adicionado 'auc', 'precision_score', 'classification_report', 'recall_score'\n",
    ") \n",
    "from sklearn.utils import resample # Importa resample para o bootstrap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# üìà I. FUN√á√ïES DE C√ÅLCULO DE M√âTRICAS AVAN√áADAS (pHI, HIT/IRPN, IC)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def calcular_phi(y_test, y_pred, y_proba):\n",
    "    \"\"\"üß† Calcula o √çndice de Confiabilidade de Previs√£o (pHi).\"\"\"\n",
    "    if y_proba is None:\n",
    "        return np.nan\n",
    "    \n",
    "    # y_proba √© a probabilidade da CLASSE 1 (coluna 1).\n",
    "    # Obtemos a probabilidade da CLASSE PREDITA.\n",
    "    y_proba_predita = np.array([\n",
    "        y_proba[i] if y_pred[i] == 1 else (1 - y_proba[i]) \n",
    "        for i in range(len(y_pred))\n",
    "    ])\n",
    "    \n",
    "    # M√©dia das probabilidades APENAS nos acertos (y_pred == y_test)\n",
    "    acertos = (y_pred == y_test)\n",
    "    probabilidades_acertos = y_proba_predita[acertos]\n",
    "    phi = np.mean(probabilidades_acertos) if len(probabilidades_acertos) > 0 else 0\n",
    "    return phi\n",
    "\n",
    "def calcular_irpn(y_test, y_pred):\n",
    "    \"\"\"üÜï Calcula o IRPN (HIT) conforme solicitado: VP / (VP + VN).\"\"\"\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    # cm √© [[TN, FP], [FN, VP]]\n",
    "    VN = cm[0, 0] \n",
    "    VP = cm[1, 1] \n",
    "    \n",
    "    return VP / (VP + VN) if (VP + VN) != 0 else np.nan\n",
    "\n",
    "def calcular_intervalo_confianca_bootstrap(y_true, y_pred, metric_func, n_iterations=1000, alpha=0.95, y_proba=None):\n",
    "    \"\"\"üìä Calcula o Intervalo de Confian√ßa (IC) por Bootstrap para m√©tricas.\"\"\"\n",
    "    stats = []\n",
    "    n_size = len(y_true)\n",
    "    indices = np.arange(n_size)\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        # Reamostragem com reposi√ß√£o\n",
    "        boot_indices = resample(indices, replace=True, n_samples=n_size)\n",
    "        \n",
    "        y_true_boot = y_true[boot_indices]\n",
    "        y_pred_boot = y_pred[boot_indices]\n",
    "        \n",
    "        # Ajuste para chamar a m√©trica corretamente, incluindo pHi\n",
    "        if metric_func.__name__ == 'calcular_phi':\n",
    "            y_proba_boot = y_proba[boot_indices] if y_proba is not None else None\n",
    "            score = metric_func(y_true_boot, y_pred_boot, y_proba_boot)\n",
    "        elif metric_func.__name__ == 'recall_score':\n",
    "            score = recall_score(y_true_boot, y_pred_boot, pos_label=1, zero_division=0)\n",
    "        elif metric_func.__name__ == 'f1_score':\n",
    "            score = f1_score(y_true_boot, y_pred_boot, pos_label=1, zero_division=0)\n",
    "        elif metric_func.__name__ == 'precision_score':\n",
    "            score = precision_score(y_true_boot, y_pred_boot, pos_label=1, zero_division=0)\n",
    "        else:\n",
    "            score = metric_func(y_true_boot, y_pred_boot)\n",
    "        \n",
    "        stats.append(score)\n",
    "\n",
    "    stats = np.array(stats)\n",
    "    p_lower = (1.0 - alpha) / 2.0\n",
    "    lower_bound = np.quantile(stats, p_lower)\n",
    "    p_upper = alpha + p_lower\n",
    "    upper_bound = np.quantile(stats, p_upper)\n",
    "    \n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# üìà II. FUN√á√ÉO PRINCIPAL DE AVALIA√á√ÉO GR√ÅFICA E M√âTRICA\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def avaliar_modelo_classificacao(y_test, y_pred, y_proba=None, model_name=\"Modelo de Classifica√ß√£o\"):\n",
    "    \"\"\"Executa a Matriz de Confus√£o, Curva ROC e calcula todas as m√©tricas + IC.\"\"\"\n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    print(f\"--- Avalia√ß√£o Final do Modelo: {model_name} ---\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # 1. Relat√≥rio de Classifica√ß√£o (vis√£o geral)\n",
    "    print(\"Relat√≥rio de Classifica√ß√£o (M√©tricas Padr√£o):\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 2. Matriz de Confus√£o\n",
    "    print(\"\\n[Gr√°fico 1] Plotando Matriz de Confus√£o...\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    disp.plot(cmap=plt.cm.Greens, values_format='d')\n",
    "    plt.title(f'Matriz de Confus√£o - {model_name}')\n",
    "    plt.show() \n",
    "    \n",
    "    # 3. Curva ROC e AUC\n",
    "    roc_auc = np.nan\n",
    "    if y_proba is not None and len(np.unique(y_test)) == 2: \n",
    "        print(\"\\n[Gr√°fico 2] Plotando Curva ROC...\")\n",
    "        y_scores = y_proba\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "        roc_auc = auc(fpr, tpr) \n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.plot(fpr, tpr, color='green', lw=2, label=f'Curva ROC (AUC = {roc_auc:.4f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Classificador Aleat√≥rio') \n",
    "        plt.title(f'Curva ROC - {model_name}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True)\n",
    "        plt.show() \n",
    "        print(f\"‚úÖ √Årea Sob a Curva ROC (AUC): {roc_auc:.4f}\")\n",
    "    \n",
    "    # 4. M√©tricas Avan√ßadas e Intervalo de Confian√ßa (IC 95%)\n",
    "    print(\"\\n--- Resultados de Confiabilidade e Customizados (IC 95% Bootstrap) ---\")\n",
    "\n",
    "    # pHi (Confiabilidade/Confian√ßa)\n",
    "    phi = calcular_phi(y_test, y_pred, y_proba)\n",
    "    # IC para pHi (passando y_proba para a fun√ß√£o bootstrap)\n",
    "    ic_phi_low, ic_phi_high = calcular_intervalo_confianca_bootstrap(y_test, y_pred, calcular_phi, y_proba=y_proba)\n",
    "    print(f\"üß† **pHi (Confiabilidade): {phi:.4f}** (IC: [{ic_phi_low:.4f}, {ic_phi_high:.4f}])\")\n",
    "    \n",
    "    # IRPN (HIT)\n",
    "    irpn = calcular_irpn(y_test, y_pred)\n",
    "    ic_irpn_low, ic_irpn_high = calcular_intervalo_confianca_bootstrap(y_test, y_pred, calcular_irpn)\n",
    "    print(f\"üÜï **HIT (VP / (VP + VN)): {irpn:.4f}** (IC: [{ic_irpn_low:.4f}, {ic_irpn_high:.4f}])\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    return roc_auc \n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# 11. Implementa√ß√£o do Random Forest (RF) com BayesSearchCV\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"--- 11. Random Forest (RF) Otimizado com class_weight='balanced' ---\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 11.1. Definir o Espa√ßo de Busca\n",
    "search_spaces_rf = {\n",
    "    'n_estimators': [75, 90, 100, 120],\n",
    "    'max_depth': [4],\n",
    "    'min_samples_split': [10],\n",
    "    'min_samples_leaf': [2],\n",
    "    'max_features': [0.8],\n",
    "    'criterion': Categorical(['gini'])\n",
    "}\n",
    "\n",
    "# 11.2. Definir a M√©trica de Otimiza√ß√£o (Aqui, usa Precision)\n",
    "scorer_rf = make_scorer(precision_score, pos_label=1)\n",
    "\n",
    "# 11.3. Configurar e Rodar o BayesSearchCV\n",
    "rf_bayes_search = BayesSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "    search_spaces=search_spaces_rf,\n",
    "    \n",
    "    n_iter=100, \n",
    "    scoring=scorer_rf,\n",
    "    \n",
    "    cv=10, \n",
    "    n_jobs=5, \n",
    "    random_state=42,\n",
    "    verbose=1 \n",
    ")\n",
    "\n",
    "# Treinamento na base H√çBRIDA (balanceada e padronizada)\n",
    "try:\n",
    "    rf_bayes_search.fit(X_resampled, y_resampled) \n",
    "except NameError:\n",
    "    print(\"ERRO: As vari√°veis 'X_resampled' e 'y_resampled' n√£o foram definidas. O treinamento n√£o pode ser realizado.\")\n",
    "    raise\n",
    "    \n",
    "# 11.4. Extrair o Melhor Modelo e Par√¢metros\n",
    "modelo_rf_best = rf_bayes_search.best_estimator_\n",
    "best_params_rf = rf_bayes_search.best_params_\n",
    "best_score_rf = rf_bayes_search.best_score_\n",
    "\n",
    "print(f\"\\n‚úÖ Melhores Par√¢metros RF (Precision) Encontrados: {best_params_rf}\")\n",
    "print(f\"‚úÖ Melhor Precision (Valida√ß√£o Cruzada): {best_score_rf:.4f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# 12. Avalia√ß√£o do Random Forest no Conjunto de Teste (NOVO: PREPARA√á√ÉO PARA FUN√á√ÉO)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# 12.1. Previs√µes\n",
    "try:\n",
    "    y_pred_rf = modelo_rf_best.predict(X_teste_df)\n",
    "    y_pred_proba_rf = modelo_rf_best.predict_proba(X_teste_df)[:, 1] # Probabilidade da classe positiva (1)\n",
    "    \n",
    "    # >>> CORRE√á√ÉO CR√çTICA: Converter para NumPy arrays para evitar KeyError no Bootstrap <<<\n",
    "    y_teste_arr = np.array(y_teste) if isinstance(y_teste, (pd.Series, pd.DataFrame)) else y_teste\n",
    "    y_pred_rf_arr = np.array(y_pred_rf)\n",
    "    y_pred_proba_rf_arr = np.array(y_pred_proba_rf)\n",
    "    \n",
    "except NameError:\n",
    "    print(\"ERRO: As vari√°veis 'X_teste_df' e 'y_teste' n√£o foram definidas. A avalia√ß√£o n√£o pode ser realizada.\")\n",
    "    raise\n",
    "\n",
    "# 12.2. Chamada da Fun√ß√£o de Avalia√ß√£o (Substitui Blocos 12, 13 e 14)\n",
    "# Esta fun√ß√£o executa a avalia√ß√£o completa, incluindo Relat√≥rio, Matriz de Confus√£o, ROC, pHi e HIT.\n",
    "avaliar_modelo_classificacao(\n",
    "    y_test=y_teste_arr, \n",
    "    y_pred=y_pred_rf_arr, \n",
    "    y_proba=y_pred_proba_rf_arr, \n",
    "    model_name=\"Random Forest Otimizado\"\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# 15. Gr√°fico Adicional: Import√¢ncia de Features (Random Forest)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"--- 15. Plotando Import√¢ncia de Features (Random Forest) ---\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 15.1. Extrair as import√¢ncias e os nomes das features\n",
    "importances = modelo_rf_best.feature_importances_\n",
    "\n",
    "# ASSUME-SE QUE X_resampled EST√Å DISPON√çVEL\n",
    "try:\n",
    "    feature_names = X_resampled.columns \n",
    "except AttributeError:\n",
    "    print(\"Aviso: 'X_resampled.columns' n√£o est√° dispon√≠vel. Usando √≠ndices num√©ricos para features.\")\n",
    "    feature_names = [f'Feature {i}' for i in range(len(importances))]\n",
    "except NameError:\n",
    "    print(\"ERRO: Vari√°vel 'X_resampled' n√£o foi definida. N√£o √© poss√≠vel calcular a import√¢ncia das features.\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# Cria um DataFrame para f√°cil visualiza√ß√£o e ordena√ß√£o\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False).head(20)\n",
    "\n",
    "# 15.2. Plotagem\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(feature_importances_df['Feature'], feature_importances_df['Importance'], color='teal')\n",
    "plt.xlabel(\"Import√¢ncia de Feature (MDI)\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Top 20 Features Mais Importantes (Random Forest)\")\n",
    "plt.gca().invert_yaxis() \n",
    "plt.show() \n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 16. Otimiza√ß√£o do Threshold (Ajuste da Fronteira)\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"--- 16. Otimiza√ß√£o do Threshold (Ajuste da Fronteira) ---\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Usa as probabilidades j√° convertidas para array\n",
    "y_proba = y_pred_proba_rf_arr \n",
    "y_teste_t = y_teste_arr # Usa o array de y_teste\n",
    "\n",
    "best_threshold = 0.8\n",
    "best_f1_score = 0\n",
    "best_recall_0 = 0\n",
    "\n",
    "# Testar thresholds de 0.01 at√© 0.5 com passos de 0.01 (Foco na classe 0)\n",
    "thresholds = np.arange(0.01, 0.51, 0.01) \n",
    "\n",
    "for t in thresholds:\n",
    "    # Previs√£o bin√°ria usando o novo threshold\n",
    "    y_pred_t = (y_proba > t).astype(int)\n",
    "    \n",
    "    # Calcular o F1-Score da Classe 1\n",
    "    f1_t = f1_score(y_teste_t, y_pred_t, pos_label=1, zero_division=0)\n",
    "    \n",
    "    # Calcular o Recall da Classe 0\n",
    "    cm_t = confusion_matrix(y_teste_t, y_pred_t)\n",
    "    # A soma da linha 0 (True Negatives + False Positives) √© o total da classe 0.\n",
    "    total_class_0 = cm_t[0].sum()\n",
    "    recall_0_t = cm_t[0, 0] / total_class_0 if total_class_0 != 0 else 0\n",
    "    \n",
    "    # Objetivo: Maximizamos o Recall da Classe 0 E o F1-score da Classe 1\n",
    "    if recall_0_t > best_recall_0:\n",
    "        best_recall_0 = recall_0_t\n",
    "        best_threshold = t\n",
    "        best_f1_score = f1_t \n",
    "\n",
    "print(f\"Melhor Threshold (Priorizando Recall da Classe 0): {best_threshold:.2f}\")\n",
    "print(f\"Recall da Classe 0 com esse Threshold: {best_recall_0:.2f}\")\n",
    "print(f\"F1-Score da Classe 1 com esse Threshold: {best_f1_score:.4f}\")\n",
    "\n",
    "# Re-avaliar o modelo usando o Threshold Otimizado\n",
    "y_pred_final = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "print(\"\\n--- Relat√≥rio de Classifica√ß√£o (Threshold Ajustado) ---\")\n",
    "# Usa o y_teste array para o relat√≥rio final\n",
    "print(classification_report(y_teste_t, y_pred_final))\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.manifold import TSNE \n",
    "# ... (outras importa√ß√µes)\n",
    "\n",
    "# ... (Seu c√≥digo de setup e vari√°veis de configura√ß√£o) ...\n",
    "\n",
    "try:\n",
    "    # 1. Carregamento, 2. Defini√ß√£o de Features/Target e 3. Divis√£o em Treino/Teste\n",
    "    print(f\"--- 1. Carregando a Base de Dados: {NOME_ARQUIVO_ENTRADA} ---\")\n",
    "    base = pd.read_csv(NOME_ARQUIVO_ENTRADA) \n",
    "    X_prev = base.drop(columns=[NOME_COLUNA_TARGET])\n",
    "    y_classe = base[NOME_COLUNA_TARGET]\n",
    "    \n",
    "    # Faz a divis√£o para ter o X_treino e y_treino ANTES de tudo\n",
    "    X_treino, X_teste, y_treino, y_teste = train_test_split(\n",
    "        X_prev, y_classe, test_size=0.20, random_state=42, stratify=y_classe \n",
    "    )\n",
    "    # Resetando √≠ndice do Y de treino para plotagem\n",
    "    y_treino_tsne = y_treino.reset_index(drop=True) \n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # üÜï ETAPA 0: Visualiza√ß√£o com t-SNE no Conjunto de TREINO Original \n",
    "    # ----------------------------------------------------------------------------------\n",
    "    print(\"--- 0. Visualiza√ß√£o (t-SNE) no Conjunto de TREINO Original ---\")\n",
    "    \n",
    "    # --- Bloco de Inicializa√ß√£o TSNE (Compatibilidade de Vers√£o) ---\n",
    "    try:\n",
    "        # Tentativa 1: Conven√ß√£o moderna (pode ser o seu caso)\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000, learning_rate='auto')\n",
    "    except TypeError:\n",
    "        # Tentativa 2: Conven√ß√£o mais antiga (se 'learning_rate' ou 'max_iter' falhar)\n",
    "        print(\"Aviso: Tentando inicializa√ß√£o TSNE com conven√ß√£o de par√¢metros mais antiga.\")\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000) \n",
    "    except Exception:\n",
    "        # Tentativa 3: Inicializa√ß√£o minimalista (para vers√µes muito antigas)\n",
    "        print(\"Aviso: Revertendo para inicializa√ß√£o TSNE minimalista.\")\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    # ---------------------------------------------------------------\n",
    "        \n",
    "    # üéØ AQUI EST√Å A MUDAN√áA: Usa X_treino\n",
    "    X_treino_tsne = tsne.fit_transform(X_treino)\n",
    "    \n",
    "    # Cria um DataFrame para plotar\n",
    "    tsne_df = pd.DataFrame(data = X_treino_tsne, columns = ['Dimens√£o 1', 'Dimens√£o 2'])\n",
    "    # üéØ AQUI EST√Å A MUDAN√áA: Usa y_treino_tsne\n",
    "    tsne_df['TEM_ARTRITE'] = y_treino_tsne.astype(str)\n",
    "    \n",
    "    # Plotagem\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(\n",
    "        x=\"Dimens√£o 1\", y=\"Dimens√£o 2\",\n",
    "        hue=\"TEM_ARTRITE\",\n",
    "        palette=sns.color_palette(\"deep\", 2), \n",
    "        data=tsne_df,\n",
    "        legend=\"full\",\n",
    "        alpha=0.8\n",
    "    )\n",
    "    plt.title('t-SNE do Conjunto de Treino')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"t-SNE conclu√≠do. Verifique o gr√°fico para ver a separa√ß√£o das classes.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Continua com a Etapa 4 (Padroniza√ß√£o)\n",
    "    # ... (Seu c√≥digo da Etapa 4 em diante - Sem altera√ß√µes) ...\n",
    "  \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERRO: O arquivo '{NOME_ARQUIVO_ENTRADA}' n√£o foi encontrado.\")\n",
    "except ImportError as e:\n",
    "    if 'skopt' in str(e):\n",
    "          print(\"‚ùå ERRO: A biblioteca 'scikit-optimize (skopt)' n√£o est√° instalada. Execute: pip install scikit-optimize\")\n",
    "    elif 'seaborn' in str(e):\n",
    "          print(\"‚ùå ERRO: A biblioteca 'seaborn' n√£o est√° instalada. Execute: pip install seaborn\")\n",
    "    else:\n",
    "          print(f\"‚ùå ERRO: Verifique se todas as bibliotecas est√£o instaladas. Erro: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Ocorreu um erro durante o processamento: {e}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
